{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "Question 1: What is K-Nearest Neighbors (KNN) and how does it work in both\n",
        "classification and regression problems?\n",
        "\n",
        "Answer: K-Nearest Neighbors (KNN) is a simple, non-parametric, supervised learning algorithm used for both classification and regression by finding the 'k' closest data points to a new data point and using their properties to make a prediction. For classification, it assigns the majority class of these neighbors to the new point.\n",
        "\n",
        "How KNN Works:-\n",
        "\n",
        "Choose 'k': Select the number of neighbors (k) to consider.\n",
        "\n",
        "Calculate Distances: Compute the distance between the new data point and all points in the training dataset.\n",
        "\n",
        "Identify K-Nearest Neighbors: Find the 'k' training data points that are closest to the new point.\n",
        "\n",
        "Make a Prediction:\n",
        "\n",
        "Classification: Assign the class that is most frequent among the 'k' nearest neighbors.\n",
        "\n",
        "Regression: Predict a continuous value by averaging the target values of the 'k' nearest neighbors."
      ],
      "metadata": {
        "id": "5LE80eWb23xx"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Question 2: What is the Curse of Dimensionality and how does it affect KNN\n",
        "performance?\n",
        "\n",
        "Answer: The Curse of Dimensionality describes how high-dimensional data leads to an exponential growth in the volume of the feature space, causing data points to become sparse and making it harder to find meaningful patterns or distances. This negatively impacts KNN (K-Nearest Neighbors) because the \"nearby\" data points, crucial for KNN's distance-based predictions, become increasingly far apart and potentially misleading due to the added dimensions\n",
        "\n",
        "How the Curse of Dimensionality Affects KNN:\n",
        "\n",
        "1. Data Sparsity:\n",
        "As the number of dimensions increases, the data points spread out, making the feature space very sparse.\n",
        "\n",
        "2. Meaningless Distances:\n",
        "In a high-dimensional space, the concept of \"nearest neighbors\" becomes less meaningful because the difference in distance between the closest and furthest neighbors can be small relative to the total distance, making distance-based comparisons less reliable.\n",
        "\n",
        "3. Increased Need for Data:\n",
        "To adequately cover the exponentially larger feature space and maintain similar data density\n",
        "\n",
        "4. Overfitting:\n",
        "With too little data to represent the vast, sparse space, KNN models are more prone to overfitting.\n",
        "\n",
        "5. Computational Cost:\n",
        "Searching for nearest neighbors in a high-dimensional space is computationally more intensive\n",
        "\n",
        "6. Noisy Features:\n",
        "In high-dimensional data, the presence of noisy or irrelevant features can have a more significant impact\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "u_xLg4SY3sJx"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Question 3: What is Principal Component Analysis (PCA)? How is it different from feature selection?\n",
        "\n",
        "Answer:- Principal Component Analysis (PCA) is a dimensionality reduction technique that transforms original features into new, uncorrelated components that capture the maximum variance in the data.\n",
        "In contrast, feature selection is a process of selecting a subset of the most relevant original features, discarding the rest, to simplify a model and improve its performance. The key difference is that PCA creates new, composite features, while feature selection retains and selects from the existing ones.\n",
        "\n",
        "Principal Component Analysis (PCA)\n",
        "\n",
        "What it is:\n",
        "PCA transforms a set of high-dimensional, correlated variables into a smaller set of new, uncorrelated variables called principal components.\n",
        "\n",
        "How it works:\n",
        "It finds directions (principal components) in the data that capture the most variance, essentially a new set of axes that better represent the data's information.\n",
        "\n",
        "Purpose:\n",
        "To reduce the dimensionality of complex datasets, which simplifies data processing, improves visualization, and can help reduce noise in the data.\n",
        "\n",
        "Outcome:\n",
        "Produces new features (principal components) that are linear combinations of the original features.\n",
        "\n",
        "Interpretation:\n",
        "The new principal components can be difficult to interpret because they are mixes of the original features.\n",
        "\n",
        "\n",
        "**Feature Selection**\n",
        "What it is:\n",
        "The process of identifying and selecting a subset of the most useful original input variables for a model.\n",
        "\n",
        "How it works:\n",
        "It involves methods to rank and select features based on their relevance to the target variable, often considering their relationship to the problem being solved.\n",
        "\n",
        "Purpose:\n",
        "To remove irrelevant or redundant features, improve model explainability, reduce overfitting, and enhance model accuracy.\n",
        "\n",
        "Outcome:\n",
        "Retains a smaller set of the original features.\n",
        "\n",
        "Interpretation:\n",
        "The resulting features are original and thus more interpretable, making it easier to understand which factors influence the model's predictions"
      ],
      "metadata": {
        "id": "_0xHNFnG4ruK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Question 4: What are eigenvalues and eigenvectors in PCA, and why are they\n",
        "important?\n",
        "\n",
        "Answer: In PCA, eigenvectors identify the directions of maximum variance in the data (the principal components), while eigenvalues quantify the amount of variance along those directions. They are important because they enable dimensionality reduction by prioritizing components that retain the most information, allowing for simplification of complex datasets and more efficient analysis.\n",
        "\n",
        "**Why are they Important in PCA?**\n",
        "\n",
        "1. Dimensionality Reduction:\n",
        "PCA uses eigenvalues and eigenvectors to find the most important features (principal components) in a dataset\n",
        "\n",
        "2. Feature Extraction:\n",
        "Eigenvectors serve as the new, lower-dimensional basis for the data. This new feature set is orthogonal and decorrelated, simplifying data representation and making it easier for downstream tasks.\n",
        "\n",
        "3. Identifying Data Patterns:\n",
        "By finding the directions of maximum variance, eigenvalues and eigenvectors reveal underlying patterns and structures within the data.\n",
        "\n",
        "4. Informing Model Selection:\n",
        "The eigenvalues help in deciding how many principal components to retain. Components with small eigenvalues might contribute little to the overall variance and can be discarded, leading to a more parsimonious model."
      ],
      "metadata": {
        "id": "q5hzUOUy8gtr"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Question 5: How do KNN and PCA complement each other when applied in a single\n",
        "pipeline?\n",
        "\n",
        "Answer: PCA complements KNN in a pipeline by first reducing data dimensionality and noise, which mitigates the \"curse of dimensionality\" and the computational burden of KNN, and then by identifying and emphasizing the most discriminative features for KNN to use.\n",
        "\n",
        "**How PCA enhances KNN **\n",
        "\n",
        "1. Dimensionality Reduction:\n",
        "KNN's effectiveness is hindered by the \"curse of dimensionality,\" where performance degrades as the number of features increases. PCA transforms the data into a lower-dimensional space using principal components, effectively reducing noise and retaining the most significant variance.\n",
        "\n",
        "2. Improved Computational Efficiency:\n",
        "By reducing the number of features, PCA significantly decreases the computational cost and time required for KNN to calculate distances between data points and find neighbors.\n",
        "\n",
        "3. Noise Reduction:\n",
        "PCA can filter out noisy features by focusing on the principal components that capture the most significant variation in the data, leading to cleaner and more effective input for KNN.\n",
        "\n",
        "4. Feature Correlation Elimination:\n",
        "PCA decorrelates features, ensuring that the principal components used by KNN are orthogonal, thus preventing multicollinearity issues and providing a more stable basis for distance calculations.\n",
        "\n",
        "5. Enhanced Pattern Recognition:\n",
        "By compressing the data into its most essential components, PCA highlights underlying patterns and structures, making it easier for KNN to correctly classify data points based on their similarity in the transformed space."
      ],
      "metadata": {
        "id": "w6BFMHOl9C-G"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Question 6: Train a KNN Classifier on the Wine dataset with and without feature\n",
        "# scaling. Compare model accuracy in both cases.\n"
      ],
      "metadata": {
        "id": "T2VdoZKN9m0z"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.datasets import load_wine\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n"
      ],
      "metadata": {
        "id": "V7nvrgWhkXxZ"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "wine = load_wine()\n",
        "X = wine.data\n",
        "y = wine.target"
      ],
      "metadata": {
        "id": "dJLVB9iEY5LZ"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n"
      ],
      "metadata": {
        "id": "G5fEc5dbZBDk"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "knn1 = KNeighborsClassifier(n_neighbors=5)\n",
        "knn1.fit(X_train, y_train)\n",
        "y_pred1 = knn1.predict(X_test)\n",
        "acc1 = accuracy_score(y_test, y_pred1)\n"
      ],
      "metadata": {
        "id": "PnMJopoxZDqI"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "scaler = StandardScaler()\n",
        "X_train_scaled = scaler.fit_transform(X_train)\n",
        "X_test_scaled = scaler.transform(X_test)\n"
      ],
      "metadata": {
        "id": "heJIxJ90ZGB_"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "knn2 = KNeighborsClassifier(n_neighbors=5)\n",
        "knn2.fit(X_train_scaled, y_train)\n",
        "y_pred2 = knn2.predict(X_test_scaled)\n",
        "acc2 = accuracy_score(y_test, y_pred2)"
      ],
      "metadata": {
        "id": "2Uy47Ra2ZJ_3"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Accuracy without Scaling: \", acc1)\n",
        "print(\"Accuracy with Scaling   : \", acc2)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yLjAhmyZZNBx",
        "outputId": "dc4dbf46-01ef-4a7f-b2eb-b4030dd0445b"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy without Scaling:  0.7407407407407407\n",
            "Accuracy with Scaling   :  0.9629629629629629\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Question 7: Train a PCA model on the Wine dataset and print the explained variance\n",
        "# ratio of each principal component.\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.datasets import load_wine\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.decomposition import PCA"
      ],
      "metadata": {
        "id": "IHXwK7ICZVaF"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "wine = load_wine()\n",
        "X = wine.data\n",
        "y = wine.target"
      ],
      "metadata": {
        "id": "-dpv6At5Zrug"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "scaler = StandardScaler()\n",
        "X_scaled = scaler.fit_transform(X)\n"
      ],
      "metadata": {
        "id": "F41d8MOBZxn0"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pca = PCA()\n",
        "X_pca = pca.fit_transform(X_scaled)"
      ],
      "metadata": {
        "id": "nUJjTJ3KZ1Hf"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Explained Variance Ratio of each component:\")\n",
        "print(pca.explained_variance_ratio_)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tywSJKPaZ33P",
        "outputId": "7a1c0b11-7647-4f1e-c8bc-2de6b5643eb2"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Explained Variance Ratio of each component:\n",
            "[0.36198848 0.1920749  0.11123631 0.0706903  0.06563294 0.04935823\n",
            " 0.04238679 0.02680749 0.02222153 0.01930019 0.01736836 0.01298233\n",
            " 0.00795215]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Question 8: Train a KNN Classifier on the PCA-transformed dataset (retain top 2\n",
        "# components). Compare the accuracy with the original dataset.\n",
        "import numpy as np\n",
        "from sklearn.datasets import load_wine\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.metrics import accuracy_score"
      ],
      "metadata": {
        "id": "leWvYTW9Z6de"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "wine = load_wine()\n",
        "X = wine.data\n",
        "y = wine.target"
      ],
      "metadata": {
        "id": "G87WU64iaMIP"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n"
      ],
      "metadata": {
        "id": "2wstDjT9aO1R"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "scaler = StandardScaler()\n",
        "X_train_scaled = scaler.fit_transform(X_train)\n",
        "X_test_scaled = scaler.transform(X_test)"
      ],
      "metadata": {
        "id": "LPRyEqHgaRH5"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "knn_orig = KNeighborsClassifier(n_neighbors=5)\n",
        "knn_orig.fit(X_train_scaled, y_train)\n",
        "y_pred_orig = knn_orig.predict(X_test_scaled)\n",
        "acc_orig = accuracy_score(y_test, y_pred_orig)"
      ],
      "metadata": {
        "id": "445SGxXzaT66"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pca = PCA(n_components=2)\n",
        "X_train_pca = pca.fit_transform(X_train_scaled)\n",
        "X_test_pca = pca.transform(X_test_scaled)"
      ],
      "metadata": {
        "id": "7ZmFBsocaXFc"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "knn_pca = KNeighborsClassifier(n_neighbors=5)\n",
        "knn_pca.fit(X_train_pca, y_train)\n",
        "y_pred_pca = knn_pca.predict(X_test_pca)\n",
        "acc_pca = accuracy_score(y_test, y_pred_pca)"
      ],
      "metadata": {
        "id": "k9DgQspsabS8"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Accuracy on Original Scaled Data: \", acc_orig)\n",
        "print(\"Accuracy on PCA (2 components):   \", acc_pca)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yjWY4i3Qad-W",
        "outputId": "6e78b918-c126-4838-cba8-b7f92876e47d"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy on Original Scaled Data:  0.9629629629629629\n",
            "Accuracy on PCA (2 components):    0.9814814814814815\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Question 9: Train a KNN Classifier with different distance metrics (euclidean,\n",
        "# manhattan) on the scaled Wine dataset and compare the results.\n",
        "import numpy as np\n",
        "from sklearn.datasets import load_wine\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.metrics import accuracy_score\n"
      ],
      "metadata": {
        "id": "CSmXC61xag5B"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "wine = load_wine()\n",
        "X = wine.data\n",
        "y = wine.target"
      ],
      "metadata": {
        "id": "5wz0LkGvaz54"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=0.3, random_state=42\n",
        ")"
      ],
      "metadata": {
        "id": "VEn9zGyla6DP"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "scaler = StandardScaler()\n",
        "X_train_scaled = scaler.fit_transform(X_train)\n",
        "X_test_scaled = scaler.transform(X_test)"
      ],
      "metadata": {
        "id": "BvubHLXea9Q2"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "knn_euclidean = KNeighborsClassifier(n_neighbors=5, metric='euclidean')\n",
        "knn_euclidean.fit(X_train_scaled, y_train)\n",
        "y_pred_euclidean = knn_euclidean.predict(X_test_scaled)\n",
        "acc_euclidean = accuracy_score(y_test, y_pred_euclidean)"
      ],
      "metadata": {
        "id": "ICDPMP60bAPl"
      },
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "knn_manhattan = KNeighborsClassifier(n_neighbors=5, metric='manhattan')\n",
        "knn_manhattan.fit(X_train_scaled, y_train)\n",
        "y_pred_manhattan = knn_manhattan.predict(X_test_scaled)\n",
        "acc_manhattan = accuracy_score(y_test, y_pred_manhattan)"
      ],
      "metadata": {
        "id": "MTm4nPjybC8Z"
      },
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Accuracy with Euclidean distance: \", acc_euclidean)\n",
        "print(\"Accuracy with Manhattan distance: \", acc_manhattan)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "L0rrTjZ9bVQc",
        "outputId": "daf34fa7-cf97-4818-9f47-6b4958cbe724"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy with Euclidean distance:  0.9629629629629629\n",
            "Accuracy with Manhattan distance:  0.9629629629629629\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Question 10: You are working with a high-dimensional gene expression dataset to\n",
        "# classify patients with different types of cancer.\n",
        "# Due to the large number of features and a small number of samples, traditional models\n",
        "# overfit.\n",
        "# Explain how you would:\n",
        "# ● Use PCA to reduce dimensionality\n",
        "# ● Decide how many components to keep\n",
        "# ● Use KNN for classification post-dimensionality reduction\n",
        "# ● Evaluate the model\n",
        "# ● Justify this pipeline to your stakeholders as a robust solution for real-world\n",
        "# biomedical data\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.model_selection import StratifiedKFold, GridSearchCV, cross_val_score\n",
        "from sklearn.metrics import make_scorer, f1_score, balanced_accuracy_score\n"
      ],
      "metadata": {
        "id": "LD29Eg-5bbME"
      },
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pipe = Pipeline(steps=[\n",
        "    (\"scaler\", StandardScaler(with_mean=True, with_std=True)),\n",
        "    (\"pca\", PCA(svd_solver=\"full\", whiten=False)),\n",
        "    (\"knn\", KNeighborsClassifier())\n",
        "])"
      ],
      "metadata": {
        "id": "IUk8oIHVb30D"
      },
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "param_grid = {\n",
        "    \"pca__n_components\": [5, 10, 15, 20, 30, 50, 100],\n",
        "    \"knn__n_neighbors\": [3, 5, 7, 9, 11],\n",
        "    \"knn__metric\": [\"euclidean\", \"manhattan\"],\n",
        "    # optionally: \"knn__weights\": [\"uniform\", \"distance\"]\n",
        "}"
      ],
      "metadata": {
        "id": "v_0kQsz0b6lr"
      },
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "scorer = make_scorer(f1_score, average=\"macro\")\n",
        "\n",
        "inner_cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
        "grid = GridSearchCV(pipe, param_grid, cv=inner_cv, scoring=scorer, n_jobs=-1)\n",
        "\n",
        "outer_cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=123)\n",
        "scores = cross_val_score(grid, X, y, cv=outer_cv, scoring=scorer, n_jobs=-1)\n",
        "\n",
        "print(\"Nested CV Macro-F1: %.3f ± %.3f\" % (scores.mean(), scores.std()))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "61Slv2ssb9Rh",
        "outputId": "4a3f77af-2640-4984-8bc2-842741dff490"
      },
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Nested CV Macro-F1: 0.962 ± 0.014\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "ZgEFAZGacA3u"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}